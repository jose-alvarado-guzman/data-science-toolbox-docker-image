{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-To-End Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is demostrating an end to end Machine Learning example project to build a model of housing prices for a real state company in California. It will walk you through the foolowing steps:\n",
    "1. Frame the problem.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms. \n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase we should seek for answers to the following questions:\n",
    "1. What exactly is the business objective? \n",
    "  * To use the model output to fed another Machine Learning system.\n",
    "2. Are they any previous solutions, and if so, how accurate are they and how they were built?\n",
    "  * Complex rule based manually estimated by market experts.\n",
    "  * The current error rate is about 15%.\n",
    "3. How does the company expect to use and benefit from this model?\n",
    "  * Decrease the cost, time and error of the current system.\n",
    "4. What data sources are currently available?\n",
    "  * California Census Data, containing metrics such as the population, median income, median housing price, and so on for each block group in California.\n",
    "5. Is it a supervised, unsupervised or reinforcement learning?\n",
    "  * Supervised learning since the data contained the labels.\n",
    "6. What king of task it is?\n",
    "  * A Multivariate Regression task since the model needs to predict a continous value out of multiple features.\n",
    "7. Should we use batch learning or online learning techniques?\n",
    "  * Batch Learning since there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Performace Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It measures the standard deviation of the errors the system makes in its predictions. The mathematical formula to calculate the RMSE is the following:\n",
    "\n",
    "$RMSE(X,f) = \\sqrt[2]{\\frac{1}{n}\\sum_{i=1}^{n}(f(x^{(i)}) - y^{(i)})^2}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$X$: is a matrix of n rows (instances) and m columns (features).\n",
    "\n",
    "$f$: is the predicting function or model.\n",
    "\n",
    "$x^{(i)}$: is the row $i$ of the matrix $X$.\n",
    "\n",
    "$f(x^{(i)})$: is the predicted value for the point $x^{(i)}$ in the $m$ dimentional space.\n",
    "\n",
    "$y^{(i)}$: is the real value of the for the row $i$ of the matrix $X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach is assuming that the Machine Learning System to be fed by our model is expecting a continous quantitative value, reason why we classified the task as a regression. What if they are actually expecting a range or interval instead of a value. In this case our task would have being a classification instead of regression. Let supposed that our assumption was comfirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tarfile import TarFile\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler, label_binarize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"/data\"\n",
    "FILE_NAME='housing.tgz'\n",
    "\n",
    "def fetch_data(dataFile):\n",
    "    with TarFile.open(dataFile) as compress_file:\n",
    "         # Decompressing the file and loading it into a Python Data Frame\n",
    "        file = compress_file.extractfile(compress_file.members.pop())\n",
    "        data=pd.read_csv(file)\n",
    "    return(data)\n",
    "\n",
    "data=fetch_data(os.path.join(DATA_DIR,FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 10 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first 10 observations it seems that all features are numeric with the exception of ocean_proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features data types and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we can observed the following:\n",
    "* The dataset contains a total of 20,640 observations and 10 features\n",
    "* All features are numberic (float) except ocean proximity\n",
    "* The only feature with missing values is total_bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe function of the Data Frame object provides several statistics including the count (frequency), min, max, mean, standard deviation (std) and the first, second and third quartiles (25% 50%, 75% percentiles respectively). From these results we can observed the following:\n",
    "* The median_income feature was scaled.\n",
    "* The housing_median_age and median_house_value was capped.\n",
    "* The quartiles of these features seems to indicate that their distributions are not normal but rather they have heavy tails.\n",
    "* All the features have different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization confirmed the heavy tails (right skewed) in the features distribution and that the median_house_value and median_house age features were capped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=data.ocean_proximity.value_counts()\n",
    "pd.DataFrame({'Labels':counts.index,\n",
    "              'Frequency':counts.values,\n",
    "              \"Relative Frequency\":np.divide(counts.values,data.shape[0])\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.2,random_state=100)\n",
    "train_set = train.copy()\n",
    "test_set = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Sampling Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax1.hist(data.median_house_value,bins=50,label='Data Set')\n",
    "ax1.hist(train_set.median_house_value,bins=50,label='Train Set')\n",
    "ax2.hist(data.median_income,bins=50,label='Data Set')\n",
    "ax2.hist(train_set.median_income,bins=50,label='Train Set')\n",
    "ax1.legend(loc='best')\n",
    "ax2.legend()\n",
    "ax1.set_title('Median House Value')\n",
    "ax2.set_title('Median Income Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the previous visualization that both variables are equally distributed in the training set when compared with the full data set. This indicates that our training set is representative of our full dataset. However, let also implement a stratified sampling by median income and determine which of the two sampling methodologies has the lower sampling bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to discretizes the median income feature to create categories based on quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'median_income_cat']=pd.qcut(data.median_income,10)\n",
    "data.median_income_cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's performed the stratified sampling based of the median house value categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.20,random_state=100)\n",
    "for train_index, test_index in split.split(data,data.median_income_cat):\n",
    "    strat_train_set = data.iloc[train_index]\n",
    "    strat_test_set = data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the sampling error for both the simple ramdom sample and the stratified sample. To do this we first need to create the same median house value categories for our simple random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.loc[:,'median_income_cat']=pd.cut(train_set.median_income,data.median_income_cat.values.categories)\n",
    "sampling_error=pd.DataFrame({'Full_Dataset':data.median_income_cat.value_counts()/len(data),\\\n",
    "                             'Random_Sample':train_set.median_income_cat.value_counts()/len(train_set),\\\n",
    "                             'Strat_Sample':strat_train_set.median_income_cat.value_counts()/len(strat_train_set)})\n",
    "sampling_error.loc[:,'Random_Error(%)']=np.divide(np.subtract(sampling_error.Random_Sample,sampling_error.Full_Dataset),\n",
    "                                                  sampling_error.Full_Dataset)*100\n",
    "sampling_error.loc[:,'Strat_Error(%)']=np.divide(np.subtract(sampling_error.Strat_Sample,sampling_error.Full_Dataset),\n",
    "                                                 sampling_error.Full_Dataset)*100\n",
    "sampling_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample errors suggests that the stratified sampling has smaller bias than the random sample. Therefore, we will continue working with the stratified training and testing datasets. Before moving on let’s remove the recent added median house value category feature from all the datasets we added to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in[data,train_set,strat_test_set,strat_train_set]:\n",
    "    del set['median_income_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and Visualize the Training Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set_copy = strat_train_set.copy()\n",
    "strat_train_set_copy.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=strat_train_set_copy[\"population\"]/100, \\\n",
    "                    figsize=(15,10),label=\"population\", c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matric = strat_train_set_copy.corr()\n",
    "corr_matric.median_house_value.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set_copy.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1,figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reveals the following: \n",
    "1. The correlation is indeed very strong\n",
    "2. There are more median house value caps than we noticed earlier at 500,000 dollars. For instance, notice the horizontal line around 450,000 and another around 350,000. \n",
    "\n",
    "We might need to consider removing the corresponding block groups to prevent learning algorithms from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct new features out of the existing ones and look at their correlation with the median house value feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set_copy[\"rooms_per_household\"] = strat_train_set_copy[\"total_rooms\"]/strat_train_set_copy[\"households\"]\n",
    "strat_train_set_copy[\"bedrooms_per_room\"] = strat_train_set_copy[\"total_bedrooms\"]/strat_train_set_copy[\"total_rooms\"]\n",
    "strat_train_set_copy[\"population_per_household\"]=strat_train_set_copy[\"population\"]/strat_train_set_copy[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set_copy.corr().median_house_value.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new bedrooms_per_room attribute has a higher correlation with the median_house_value feature than the total_bedrooms or total_rooms features. Since it's correlation value is negative, this indicates that houses with a lower bedroom/room ratio tend to be more expensive. The rooms_per_household feature is also more informative than the total_rooms feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DF = strat_train_set.drop('median_house_value',axis=1)\n",
    "Y = strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we noticed that the feature total_bedrooms include missing values. Let's see how many observations in the training have the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total_bedrooms missing values: {0}'.format(X_DF.total_bedrooms.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Machine Learning algorithms cannot work with features with missing values, so we need to handle them before training the algorithms. To handle these missing values, we have the following three options:\n",
    "1. Removing the instances with the missing values.\n",
    "2. Removing the entire feature with missing values from the training and test sets.\n",
    "3. Impute the missing values.\n",
    "\n",
    "Let's impute the missing values of all features by using the median of the respective feature. To accomplish this the following steps were used:\n",
    "1. Create a data frame with only the quantitative features by dropping the ocean_proximity feature\n",
    "2. Use the SimpleImputer class with the strategy set to median to calculate the median of all the quantitative features\n",
    "3. Use the transform method of this class to replace the missing values of the features with the corresponding median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DF_Quant = strat_train_set.drop('ocean_proximity',axis=1) \n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_DF_Quant)\n",
    "X_DF_Quant = pd.DataFrame(imputer.transform(X_DF_Quant),columns=X_DF_Quant.columns)\n",
    "print('total_bedrooms missing values: {0}'.format(X_DF_Quant.total_bedrooms.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Machine Learning Algorithms are executed on a Matrix of numbers. Currently our dataset contains one non numeric feature. So we need to find a numeric representation for this feature. The LabelBinarizer class does exactly that by representing the feature as a matrix. The columns of this matrix represent the different values of the feature and the rows represent the observations. The value observed in a particular observation get encoded as a 1 and all other cell entries in that row get encoded as 0. This is call **one-hot-encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "ocean_prox = strat_train_set.ocean_proximity.copy()\n",
    "ocean_prox_encoded = encoder.fit_transform(ocean_prox)\n",
    "ocean_prox_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your own transformers so that they work seamlessly with other Scikit-Learn functionalities (such as pipelines), we need to create a class that implements the following 3 methods:\n",
    "1. <u>fit</u>: returning self\n",
    "2. <u>transform</u>: to apply the learned transformation\n",
    "3. <u>fit_transform</u>: to learn and apply the trasformation. We can inherit this method by extending the TransformerMixin class. If we also extend the BaseEstimator and avoid the \\*args and \\*\\*kargs in the constructor, we will get additional methods like the get_params and set_params methods.\n",
    "\n",
    "To illustrate this let's create a custom transformer to add the additional features we calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        \n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        self.rooms_ix = pd.Index(strat_train_set.columns).get_loc('total_rooms')\n",
    "        self.bedrooms_ix = pd.Index(strat_train_set.columns).get_loc('total_bedrooms')\n",
    "        self.population_ix = pd.Index(strat_train_set.columns).get_loc('population')\n",
    "        self.household_ix = pd.Index(strat_train_set.columns).get_loc('households')\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:,self.rooms_ix] / X[:,self.household_ix] \n",
    "        population_per_household = X[:,self.population_ix] / X[:,self.household_ix] \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,self.bedrooms_ix] / X[:,self.rooms_ix]\n",
    "            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) \n",
    "X_Quant_Extra_Attr = attr_adder.transform(X_DF_Quant.values)\n",
    "X_Quant_Extra_Attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is one of the most important transformations that needs to be apply to the data prior to training any learning algorithms. The reason for this is that must Machine Learning algorithms don’t perform well with quatitative attributes that have very different scales. The two must common methods to scale attribute are the following:\n",
    "1. Min-Max Scaling (Normalization): The attributes are scaled to fall in the interval [0,1]. The mathematical formula to scale the value i of the attribute X is given by $\\frac{X_i - min(X)}{max(X) - min(X)}$\n",
    "2. Standardization: Scale the attribute so that the mean is equal to 0 and the variance is equal to 1. The mathematical formula to scale the value i of the attribute X is given by $\\frac{X_i - mean(X)}{std(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_Quant_Extra_Attr_Scaled = std_scaler.fit_transform(X_Quant_Extra_Attr)\n",
    "print(np.apply_along_axis(np.mean,0,X_Quant_Extra_Attr_Scaled))\n",
    "print(np.apply_along_axis(np.std,0,X_Quant_Extra_Attr_Scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "So far we had applied the following transformations to our training set:\n",
    "1. Separeted the quantitative and qualitative attributes.\n",
    "2. Inputed missing values.\n",
    "3. Added additional attributes.\n",
    "4. Scaled the quantitatives attributes.\n",
    "\n",
    "To applied this trasnformation in order and in a more automated way we can use the transformations pipelines available in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class MyLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,classes):\n",
    "        self.classes_ = classes\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return label_binarize(x,classes=self.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_pipeline = Pipeline([('selector',AttributeSelector(list(X_DF.select_dtypes(exclude=['object','category'])))),\n",
    "                            ('inputer',SimpleImputer(strategy='median')),\n",
    "                           ('attr_adder',CombinedAttributesAdder())\n",
    "                           ,('std_scaler',StandardScaler())])\n",
    "\n",
    "qual_pipeline = Pipeline([('selector',AttributeSelector(list(X_DF.select_dtypes(include=['object','category'])))),\n",
    "                           ('label_binarizer',MyLabelBinarizer(X_DF.ocean_proximity.unique()))])\n",
    "\n",
    "data_prep_pipeline = FeatureUnion(transformer_list=[('quant_pipeline',quant_pipeline),\n",
    "                                               ('qual_pipeline',qual_pipeline)])\n",
    "X_Prepared = data_prep_pipeline.fit_transform(X_DF)\n",
    "print(X_Prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating using the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Linear Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_Prepared, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script select 5 random observation from the training set used them to make predictions. These predictions along with the real values are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_index = random.sample(list(X_DF.index),5)\n",
    "small_sample = X_DF.loc[small_index,:]\n",
    "small_sample_prepared = data_prep_pipeline.fit_transform(small_sample)\n",
    "print('Prediction: {0}'.format(linear_reg.predict(small_sample_prepared)))\n",
    "print('Real Values: {0}'.format(list(Y[small_index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the predictions using the entire training set and calculate the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred = linear_reg.predict(X_Prepared)\n",
    "linear_rmse = np.sqrt(mean_squared_error(linear_pred,Y))\n",
    "print('Linear model training error: {0}'.format(linear_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error of the Linear Regression model is approximately \\$68,247. This error is not a satisfying one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_Prepared,Y)\n",
    "tree_pred = tree_reg.predict(X_Prepared)\n",
    "tree_rmse = np.sqrt(mean_squared_error(tree_pred,Y))\n",
    "print('Decision Tree training error: {0}'.format(tree_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can this be the best model since it has a 0 RMSE? It is possible that this model has badly overfit the data. How can we verify this? We don't want to touch the test set until we identified the most accurate model that we are confident about, so we need to use part of the training set for training, and part for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn cross-validation features expect an utility function (greater is better) rather than a cost function (lower is better). For this reason the scoring parameter is set to the negative mean square error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class is used to train and cross validate Machine Learning models and stored the results in an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Model():\n",
    "    \n",
    "    def __init__(self,X,Y,model,folds):\n",
    "        self.model_reg_ = model.fit(X,Y)\n",
    "        self.model_pred = model.predict(X)\n",
    "        self.model_rmse = np.sqrt(mean_squared_error(self.model_pred,Y))\n",
    "        self.model_cross_scores = cross_val_score(self.model_reg_,X,Y,scoring='neg_mean_squared_error',cv=folds)\n",
    "        self.model_cross_rmse = np.sqrt(-self.model_cross_scores)\n",
    "        self.model_stats=pd.Series(self.model_cross_rmse).describe()[1:]\n",
    "        self.model_stats=pd.Series({'training_error':self.model_rmse}).append(self.model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script is using the previous class to train and cross validate the following 4 models:\n",
    "1. Linear Regression\n",
    "2. Decision Tree\n",
    "3. Support Vertor Machine with a linear kernel\n",
    "4. Random Forest\n",
    "\n",
    "The statistics of these models are then combined into a pandas Data Frame object. This following code block will take several minutes since is training 4 models and validate them using a 4 fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = ML_Model(X_Prepared,Y,tree_reg,4)\n",
    "linear_model = ML_Model(X_Prepared,Y,linear_reg,4)\n",
    "svm_model = ML_Model(X_Prepared,Y,SVR(kernel='linear'),4)\n",
    "random_forest_model = ML_Model(X_Prepared,Y,RandomForestRegressor(random_state=42),4)\n",
    "pd.DataFrame({'Linear Regression':linear_model.model_stats,\n",
    "              'Decision Tree':decision_tree_model.model_stats,\n",
    "              'SVM':svm_model.model_stats,\n",
    "              'Random Forest':random_forest_model.model_stats\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we can observed the following facts:\n",
    "* The Decision Tree cross validation mean error is far from it's training error. This is indicating that this model is heavily overfitting the training data.\n",
    "* The Linear model performed better than the Decision Tree according to the cross-validation results.\n",
    "* Random Forests seems very promising since it's cross validation error is smaller than all other models. However, this model is still overfitting the training set since the training error is lower than the cross validation error. Possible solutions for this are to simplify the model, constrain it (regularization), or get more training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV evaluate all the possible combinations of hyperparameter values we want to experiment with and using cross-validationto it will determine the optimal one. The following code searches for the best combination of hyperparameter values for the Random Forest Regressor. When you have no idea what value to set hyperparameter to, a simple approach is to try out consecutive powers of 10 or a smaller number for higher fine-grained search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [10, 20, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [10, 20], 'max_features': [2, 4, 6]},\n",
    "  ]\n",
    "\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(random_forest_model.model_reg_, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(X_Prepared, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid search explored 18 combinations of Random Forest Regressor hyperparameter values and trained each model five times. In other words, we performed 18 × 5 = 90 rounds of training. The following code obtains the combination of hyperparameters with the lowest cross validation estimated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation errors are available in the instance variable cv_results as the foolowing code illustrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv_results=grid_search.cv_results_\n",
    "for mean_score, params in zip(gridsearch_cv_results['mean_test_score'],gridsearch_cv_results['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use **RandomizedSearchCV** instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "1. If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).\n",
    "2. You have more control over the computing cost you want to allocate to hyperparameter search, simply by setting the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the best models and their errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the feature weight on making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_features=list(X_DF.drop(['ocean_proximity'],axis=1).columns)\n",
    "extra_features=['rooms_per_household','population_per_household','bedrooms_per_room']\n",
    "qualitative_features=list(encoder.classes_)\n",
    "all_features=quantitative_features+extra_features+qualitative_features\n",
    "feature_weights=grid_search.best_estimator_.feature_importances_\n",
    "sorted(zip(feature_weights, all_features), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these results we may want to try dropping some of the less important features. For instance, only one ocean_proximity category seems useful (INLAND), so we could consider dropping the others.\n",
    "We should also look at the specific errors that model makes, then try to understand why it makes them and what could we do to fix them. Perhaps by adding extra features or, on the contrary, getting rid of uninformative ones, dealing with outliers. The following class is used to select the Top k features with higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def top_k_feature_index(self,arr, k):\n",
    "        return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "    \n",
    "    def __init__(self, feature_weights, k):\n",
    "        self.feature_weights = feature_weights\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = self.top_k_feature_index(self.feature_weights, self.k)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    ('preparation', data_prep_pipeline),\n",
    "    ('feature_selection', TopKFeatureSelector(feature_weights, 8))\n",
    "])\n",
    "X_Prepared=full_pipeline.fit_transform(X_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Model(X_Prepared,Y,RandomForestRegressor(random_state=42),4).model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that the Random Forrest Model performs slightly better when removing the less important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest error was Random forest with the parameter tunning. Therefore, let's evaluate this model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_Test = strat_test_set.drop(\"median_house_value\", axis=1) \n",
    "Y_Test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_Test_Prepared = data_prep_pipeline.transform(X_Test)\n",
    "final_predictions = final_model.predict(X_Test_Prepared)\n",
    "final_mse = mean_squared_error(Y_Test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is slightly worse than what we measured using cross-validation and that is usually the case in preactice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch, Monitor, and Maintain Your System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready for production, we can do this by following these steps:\n",
    "1. Plugging the production input data sources and writing tests. \n",
    "2. Write monitoring code to check your system’s live performance at regular time intervals and trigger alerts when there are performance decays. Evaluating your system’s performance will require sampling the system’s predictions and evaluating them. This will generally require a human analysis. These analysts may be field experts, or workers on a crowdsourcing platform (such as Amazon Mechanical Turk or CrowdFlower). Either way, you need to plug the human evaluation pipeline into your system.\n",
    "3. Evaluate the system’s input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the inputs is particularly important for online learning systems.\n",
    "4. Retrain your models on a regular basis using fresh data. You should automate this process as much as possible. If you don’t, you are very likely to refresh your model only every six months (at best), and your system’s performance may fluctuate severely over time. If your system is an online learning system, you should make sure you save snapshots of its state at regular intervals so you can easily roll back to a previously working state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_DS_Training",
   "language": "python",
   "name": "python3_ds_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
