{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraudulent Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to demonstrate that adding graph features as part of a Machine Learning pipeline often results in more accurate predictions. To accomplish this, weâ€™ll use the [PaySim](https://github.com/voutilad/paysim-demo) dataset and go thru the following steps:\n",
    "1. Build Binary Classifier, using traditional Machine Learning (ML) features, to detect fraudulent transactions.\n",
    "2. Retraining the Binary Classifier on an enhance set of features (by adding graph features).\n",
    "3. Compare the performance measures on both models.\n",
    "4. Looking at the features weight or importance in the model.\n",
    "5. Use Regularization to select the most importance features.\n",
    "6. Discuss the Precision/Recall Threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from neo4j import GraphDatabase\n",
    "from multiprocessing import Pool,cpu_count\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler, label_binarize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,roc_auc_score,precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is use to encapsulate the connection to Neo4j Database and the execution of queries. It has the following instance variables:\n",
    "1. _driver: Neo4j driver instance\n",
    "2. query: Cypher query that was ran last\n",
    "3. data: Pandas Data Frame object containing the results of a Cypher query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    \n",
    "    def __init__(self, uri, user, password,encrypted=False):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password),encrypted=encrypted)\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "        \n",
    "    def getData(self,query):\n",
    "        self.query = query\n",
    "        with self._driver.session() as session:\n",
    "            results = session.run(query)\n",
    "        self.data = pd.DataFrame(results.values(),columns=results.keys())\n",
    "    \n",
    "    def runQuery(self,query):\n",
    "        self.query = query\n",
    "        with self._driver.session() as session:\n",
    "            session.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is use to split a dataset into training and testing. It has the following instance variables:\n",
    "1. original_data: Original Data Frame passed as an argument\n",
    "2. data: Original dataset after deleting the attributes that are not going to be used as ML features\n",
    "3. train_set and test_set: Resulting Dataframes after train & test spliting\n",
    "4. X_Train_DF, X_Test_DF: Train and test Dataframes after removing the fraud labels\n",
    "5. Y_Train, Y_Test: Train and test fraud labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets():\n",
    "    \n",
    "    def __init__(self,data,test_size=.25):\n",
    "        self.original_data = data\n",
    "        self.data = data.drop(['data_chunk','txOrder','fromId','toId'],axis=1)\n",
    "        self.data.previousTxType.fillna('First',inplace=True)\n",
    "        train_set, test_set = train_test_split(self.data,test_size=test_size,random_state=42)\n",
    "        self.train_set = train_set.copy()\n",
    "        self.test_set = test_set.copy()\n",
    "        self.X_Train_DF = self.train_set.drop('fraudulentTx',axis=1)\n",
    "        self.Y_Train = self.train_set['fraudulentTx'].copy()\n",
    "        self.X_Test_DF = self.test_set.drop('fraudulentTx',axis=1)\n",
    "        self.Y_Test = self.test_set['fraudulentTx'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttributeSelector Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is use to select the attributes from the dataset that are quantitative or qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, quantitative = True): \n",
    "        self.quantitative = quantitative\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.quantitative:\n",
    "            self.attribute_names = list(X.select_dtypes(exclude=['object','category']).columns)\n",
    "        else:\n",
    "            self.attribute_names = list(X.select_dtypes(include=['object','category']).columns)\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyLabelBinarizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is use to create a one hot encoding for all the qualitative attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        transformations = []\n",
    "        for col in range(x.shape[1]):\n",
    "            transformations.append(label_binarize(x[:,col],classes=np.unique(x[:,col])))\n",
    "        return np.hstack(transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to execute a parallel computation of the minimum, mean and maximum of the previous 7 transaction and add these features to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevTxFeatures(data,lastK=7):\n",
    "    for i in range(data.shape[0]):\n",
    "        last7TxAmount = data.iloc[:i+1,2].tail(lastK)\n",
    "        data.iloc[i,7] = last7TxAmount.min()\n",
    "        data.iloc[i,8] = last7TxAmount.mean()\n",
    "        data.iloc[i,9] = last7TxAmount.max()\n",
    "    return(data)\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    return pd.concat(ret_list)\n",
    "\n",
    "def groupApply(df):\n",
    "    return df.groupby('fromId').apply(prevTxFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j Database Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'bolt://neo4jdb:7687'\n",
    "graph = Database(uri,user='neo4j',password='DS_Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the previous transaction features describe previously take approximately 15 minutes. In order to avoid waiting for this process to complete, a Data Frame containing these features were persisted as a pickle file. The following code cell is loading this Data Frame into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/data/paysim_transactions.pkl.gzip',compression='gzip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 code cells can be skip, they only demonstrate how the previous ML features were obtained and persisted to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cypherQuery = '''\n",
    "MATCH(c:Client)-[:PERFORMED]->(t:Transaction)-[:TO]->(n)\n",
    "WITH t,c,labels(t)[0] AS txType,t.amount AS txAmount,t.globalStep AS txOrder,\n",
    "\tt.fraud AS fraudulentTx,id(n) AS toId\n",
    "OPTIONAL MATCH(t)<-[:NEXT]-(pt:Transaction)\n",
    "RETURN id(c) AS fromId,txType,txAmount,txOrder,fraudulentTx,toId,labels(pt)[0] AS previousTxType\n",
    "ORDER BY id(c),txOrder\n",
    "'''\n",
    "graph.getData(cypherQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "data = graph.data.assign(min7PrevTxAmount=0.0,mean7PrevTxAmount=0.0,max7PrevTxAmount=0.0)\n",
    "data[\"data_chunk\"] = data[\"fromId\"].mod(cpu_count() * 3)\n",
    "data = applyParallel(data.groupby('data_chunk'),groupApply)\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.to_pickle('./paysim_transactions.pkl.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = Datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.train_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.X_Train_DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "missing = {}\n",
    "for col in ml_data.X_Train_DF.select_dtypes(include=['object','category']).columns:\n",
    "    frequency[col] = ml_data.X_Train_DF[col].value_counts()\n",
    "    missing[col] = ml_data.X_Train_DF[col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Labels Sampling Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.data.fraudulentTx.value_counts()/ml_data.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.Y_Train.value_counts()/len(ml_data.Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Training Set for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_pipeline = Pipeline([('selector',AttributeSelector()),\n",
    "                           ('std_scaler',StandardScaler())\n",
    "                          ])\n",
    "\n",
    "qual_pipeline = Pipeline([('selector',AttributeSelector(quantitative=False)),\n",
    "                          ('label_binarizer',MyLabelBinarizer())\n",
    "                         ])\n",
    "\n",
    "data_prep_pipeline = FeatureUnion(transformer_list=[('quant_pipeline',quant_pipeline),\n",
    "                                               ('qual_pipeline',qual_pipeline)])\n",
    "X_Prepared = data_prep_pipeline.fit_transform(ml_data.X_Train_DF)\n",
    "print(X_Prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier (SGD) Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is training and SGD model and evaluating its accuracy using a 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_Prepared, ml_data.Y_Train)\n",
    "cross_val_score(sgd_clf, X_Prepared, ml_data.Y_Train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has correctly classified 99.7 % of the transactions. However, this is simply because only about 1% of the transactions are labeled as fraudulent. So, a model that always classifies a transaction as non-fraudulent, it will correctly classify 99% of the transactions. Beats Nostradamus!\n",
    "\n",
    "This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (some labels are much more frequent than others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general is to estimate how frequent the model incorrectly classifies the transactions. Each row of this matrix represents the observations, while each column represents the predictions.\n",
    "\n",
    "Observations/Predictions| Negative | Positive   |\n",
    ":---------|:------------:|:----------|\n",
    "Negative | TN | FP|\n",
    "Positive | FN| TP|\n",
    "\n",
    "Where:\n",
    "* TP = True Positive\n",
    "* FP = False Positive\n",
    "* FN = False Negative\n",
    "* TN = True Negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(sgd_clf,X_Prepared,ml_data.Y_Train,cv=5)\n",
    "ml_conf_matrix = confusion_matrix(ml_data.Y_Train,predictions)\n",
    "ml_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the confusion matrix provides a lot of information, there are more concise metrics available.\n",
    "\n",
    "* __Precision__ is the accuracy of the positive predictions. \n",
    "\n",
    "  $precision =\\frac{TP}{TP + FP}$\n",
    "  \n",
    "    \n",
    "* __Recall__, also called sensitivity or true positive rate (TPR)), is the ratio of positive instances that were correctly classified \n",
    "\n",
    "    $recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "* __$F_1$ Score__ is the harmonic mean of precision and recall\n",
    "\n",
    "    $F_1 = \\frac{TP}{TP + \\frac{FP + FN}{2}}$\n",
    "    \n",
    "* __ROC AUC__ is the Receiver Operating Characteristic (ROC) Area Under the Curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_measures={}\n",
    "performance_measures['precision'] = precision_score(ml_data.Y_Train,predictions)\n",
    "performance_measures['recall'] = recall_score(ml_data.Y_Train,predictions)\n",
    "performance_measures['f1'] = f1_score(ml_data.Y_Train, predictions)\n",
    "performance_measures['roc_auc'] = roc_auc_score(ml_data.Y_Train,predictions)\n",
    "pd.Series(performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Powered ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is creating a monopartite in-memory graph named Transactions using a Cypher projection. The nodes on this in-memory graph represents clients and the relationships represents transactions among them. The weight in these edges are the total amount of transferred between clients. This graph is then used to execute Page Rank, Betweenness, Triangle Count and Closeness algorithms and their results are stored in the Neo4j Database as properties of the Client nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedGraphQuery = '''\n",
    "CALL gds.graph.create.cypher('Transactions',\n",
    "\t'MATCH(n) WHERE n:Client OR n:Bank OR n:Merchant RETURN id(n) AS id',\n",
    "    'MATCH(c:Client)-[:PERFORMED]->(t:Transaction)-[:TO]->(n)\n",
    "\tRETURN id(c) AS source, id(n) AS target,sum(t.amount) AS totalAmount'\n",
    ");\n",
    "'''\n",
    "\n",
    "pageRankQuery='''\n",
    "CALL gds.pageRank.write('Transactions',{relationshipWeightProperty:'totalAmount',writeProperty:'pageRankScore'});\n",
    "'''\n",
    "\n",
    "betweennessQuery='''\n",
    "CALL gds.betweenness.write('Transactions',{writeProperty:'betweennessScore'});\n",
    "'''\n",
    "\n",
    "triangleCountQuery='''\n",
    "CALL gds.triangleCount.write('Transactions',{writeProperty:'triangleCount'})\n",
    "'''\n",
    "\n",
    "closenessQuery = '''\n",
    "CALL gds.alpha.closeness.write('Transactions',{writeProperty:'closeness'})\n",
    "'''\n",
    "\n",
    "removeNamedGraph='''\n",
    "CALL gds.graph.drop('Transactions')\n",
    "'''\n",
    "\n",
    "graph.runQuery(namedGraphQuery)\n",
    "graph.runQuery(pageRankQuery)\n",
    "graph.runQuery(betweennessQuery)\n",
    "graph.runQuery(triangleCountQuery)\n",
    "graph.runQuery(closenessQuery)\n",
    "graph.runQuery(removeNamedGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is retrieving all the results algorithms ran previously as a Pandas Data Frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphFeaturesQuery='''\n",
    "MATCH(n)\n",
    "WHERE n:Client OR n:Merchant OR n:Bank\n",
    "RETURN id(n) AS id,n.pageRankScore AS pageRank,n.betweennessScore AS betweenness,n.triangleCount AS triangleCount,\n",
    "    n.closeness AS closeness\n",
    "'''\n",
    "graph.getData(graphFeaturesQuery)\n",
    "graph.close()\n",
    "graphFeatures = graph.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining ML Features and Graph Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures = ml_data.original_data.merge(graphFeatures,left_on='toId',right_on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ml_data = Datasets(allFeatures.drop('id',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ml_data.data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data for ML, Training and Obtaining Accuracy for Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Prepared = data_prep_pipeline.fit_transform(graph_ml_data.X_Train_DF)\n",
    "sgd_clf.fit(X_Prepared, graph_ml_data.Y_Train)\n",
    "cross_val_score(sgd_clf, X_Prepared, graph_ml_data.Y_Train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Power ML Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are obtaining the Graph Powered ML model Confusion Matrix and comparing it with the Traditional ML model Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(sgd_clf,X_Prepared,graph_ml_data.Y_Train,cv=5)\n",
    "graph_ml_conf_matrix = confusion_matrix(graph_ml_data.Y_Train,predictions)\n",
    "print(graph_ml_conf_matrix)\n",
    "print('Change')\n",
    "print(graph_ml_conf_matrix - ml_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed in the matrix above, a total of 1,322 and 573 False Positive and False Negative were correctly classified using the Graph Powered ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Powered ML Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_performance_measures={}\n",
    "graph_performance_measures['precision'] = precision_score(graph_ml_data.Y_Train,predictions)\n",
    "graph_performance_measures['recall'] = recall_score(graph_ml_data.Y_Train,predictions)\n",
    "graph_performance_measures['f1'] = f1_score(graph_ml_data.Y_Train, predictions)\n",
    "graph_performance_measures['roc_auc'] = roc_auc_score(graph_ml_data.Y_Train,predictions)\n",
    "pd.Series(graph_performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Traditional ML and Graph Powered ML Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(performance_measures,name='ML'),pd.Series(graph_performance_measures,name='Graph_ML')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Weight (Importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_features = data_prep_pipeline.get_params()['quant_pipeline'].get_params()['steps'][0][1].attribute_names\n",
    "qualitative_features = data_prep_pipeline.get_params()['qual_pipeline'].get_params()['steps'][0][1].attribute_names\n",
    "all_features = quantitative_features\n",
    "for col in qualitative_features:\n",
    "    all_features = all_features + [col + '_' + x for x in list((graph_ml_data.data[col].unique()))]\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associating each feature with its corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(np.abs(sgd_clf.coef_[0]), all_features), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization of the Graph Power ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42,penalty='elasticnet')\n",
    "sgd_clf.fit(X_Prepared, graph_ml_data.Y_Train)\n",
    "predictions = cross_val_predict(sgd_clf,X_Prepared,graph_ml_data.Y_Train,cv=5)\n",
    "graph_ml_conf_matrix - confusion_matrix(graph_ml_data.Y_Train,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(np.abs(sgd_clf.coef_[0]), all_features), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Powered ML Model Test Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test_Prepared = data_prep_pipeline.fit_transform(graph_ml_data.X_Test_DF)\n",
    "predictions = cross_val_predict(sgd_clf,X_Test_Prepared,graph_ml_data.Y_Test,cv=5)\n",
    "test_performance_measures={}\n",
    "test_performance_measures['precision'] = precision_score(graph_ml_data.Y_Test,predictions)\n",
    "test_performance_measures['recall'] = recall_score(graph_ml_data.Y_Test,predictions)\n",
    "test_performance_measures['f1'] = f1_score(graph_ml_data.Y_Test, predictions)\n",
    "test_performance_measures['roc_auc'] = roc_auc_score(graph_ml_data.Y_Test,predictions)\n",
    "print(confusion_matrix(graph_ml_data.Y_Test,predictions))\n",
    "pd.Series(graph_performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Tradeoff\n",
    "\n",
    "The F1 score favors classifiers that have similar precision and recall. This is not always what you want, in some contexts you mostly care about precision, and in other contexts you really care about recall. As the following code cell demonstrates, it is fairly easy to create a classifier with virtually any precision you want just by setting a high enough threshold. However, a high precision classifier is not very useful if its recall is too low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Scores = cross_val_predict(sgd_clf,X_Prepared,graph_ml_data.Y_Train,cv=5,method='decision_function')\n",
    "precisions, recalls, thresholds = precision_recall_curve(graph_ml_data.Y_Train, Y_Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(17,10))\n",
    "axes[0].plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "axes[0].plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "axes[0].set_xlabel(\"Threshold\")\n",
    "axes[0].legend(loc=\"best\")\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_title('Precision & Recall Against Threshold')\n",
    "axes[1].plot(recalls,precisions)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision Against Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train_Recall_98 = (Y_Scores > -1)\n",
    "precision = precision_score(graph_ml_data.Y_Train,Y_Train_Recall_98)\n",
    "recall = recall_score(graph_ml_data.Y_Train,Y_Train_Recall_98)\n",
    "print('Precision: {0}'.format(precision))\n",
    "print('Recall: {0}'.format(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_DS_Training",
   "language": "python",
   "name": "python3_ds_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
